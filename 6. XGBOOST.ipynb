{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6859122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORTS\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    precision_recall_curve, roc_curve, roc_auc_score\n",
    ")\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Add parent directory to path for preprocessing import\n",
    "sys.path.append('scripts')\n",
    "from preprocessing import preprocess\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d07e0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RUN PREPROCESSING\n",
    "# ============================================================\n",
    "\n",
    "# Run preprocessing function directly\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = preprocess(\n",
    "    data_path='data/raw/WA_Fn-UseC_-HR-Employee-Attrition.csv'\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nFeatures: {list(X_train.columns)}\")\n",
    "print(f\"\\nClass distribution (Training):\")\n",
    "print(y_train.value_counts(normalize=True).round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231aa580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CLASS IMBALANCE - CALCULATE SCALE_POS_WEIGHT\n",
    "# ============================================================\n",
    "\n",
    "# XGBoost uses scale_pos_weight to handle imbalance\n",
    "# Recommended: (number of negative samples) / (number of positive samples)\n",
    "\n",
    "n_negative = (y_train == 0).sum()\n",
    "n_positive = (y_train == 1).sum()\n",
    "scale_pos_weight = n_negative / n_positive\n",
    "\n",
    "print(f\"Class 0 (Stay): {n_negative}\")\n",
    "print(f\"Class 1 (Leave): {n_positive}\")\n",
    "print(f\"Imbalance ratio: {scale_pos_weight:.2f}\")\n",
    "print(f\"\\nRecommended scale_pos_weight: {scale_pos_weight:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a928bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# XGBOOST HYPERPARAMETER TUNING WITH GRIDSEARCHCV\n",
    "# ============================================================\n",
    "\n",
    "# Set up StratifiedKFold\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'scale_pos_weight': [1, scale_pos_weight, scale_pos_weight * 1.5]\n",
    "}\n",
    "\n",
    "# Base classifier\n",
    "xgb_clf = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# GridSearchCV with F1 scoring\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_clf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1',\n",
    "    cv=skf,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "print(\"Starting GridSearchCV... This may take a few minutes.\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GRIDSEARCHCV RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best F1 Score (CV): {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Best estimator\n",
    "best_xgb = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcce210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# THRESHOLD OPTIMIZATION - DUAL STRATEGY\n",
    "# 1. F1-Optimized: Best balance between precision and recall\n",
    "# 2. Recall-Optimized: Minimize surprise departures\n",
    "# ============================================================\n",
    "\n",
    "# Get predicted probabilities on validation set\n",
    "y_val_proba = best_xgb.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Calculate precision-recall curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_val, y_val_proba)\n",
    "\n",
    "# ---- F1-OPTIMIZED THRESHOLD ----\n",
    "f1_scores = 2 * (precisions[:-1] * recalls[:-1]) / (precisions[:-1] + recalls[:-1] + 1e-10)\n",
    "best_f1_idx = np.argmax(f1_scores)\n",
    "f1_optimal_threshold = thresholds[best_f1_idx]\n",
    "f1_achieved_recall = recalls[best_f1_idx]\n",
    "f1_achieved_precision = precisions[best_f1_idx]\n",
    "f1_achieved_f1 = f1_scores[best_f1_idx]\n",
    "\n",
    "# ---- RECALL-OPTIMIZED THRESHOLD ----\n",
    "TARGET_RECALL = 0.80  # Catch at least 80% of leavers\n",
    "\n",
    "valid_indices = np.where(recalls[:-1] >= TARGET_RECALL)[0]\n",
    "\n",
    "if len(valid_indices) > 0:\n",
    "    best_idx = valid_indices[np.argmax(precisions[:-1][valid_indices])]\n",
    "    recall_optimal_threshold = thresholds[best_idx]\n",
    "    recall_achieved_recall = recalls[best_idx]\n",
    "    recall_achieved_precision = precisions[best_idx]\n",
    "else:\n",
    "    recall_optimal_threshold = thresholds[0]\n",
    "    recall_achieved_recall = recalls[0]\n",
    "    recall_achieved_precision = precisions[0]\n",
    "    print(f\"WARNING: Target recall {TARGET_RECALL:.0%} not achievable. Using lowest threshold.\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"THRESHOLD OPTIMIZATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'F1-OPTIMIZED THRESHOLD':^60}\")\n",
    "print(f\"Threshold: {f1_optimal_threshold:.3f}\")\n",
    "print(f\"Precision: {f1_achieved_precision:.2%}\")\n",
    "print(f\"Recall: {f1_achieved_recall:.2%}\")\n",
    "print(f\"F1 Score: {f1_achieved_f1:.3f}\")\n",
    "\n",
    "print(f\"\\n{'RECALL-OPTIMIZED THRESHOLD':^60}\")\n",
    "print(f\"Target Recall: {TARGET_RECALL:.0%}\")\n",
    "print(f\"Threshold: {recall_optimal_threshold:.3f}\")\n",
    "print(f\"Precision: {recall_achieved_precision:.2%}\")\n",
    "print(f\"Recall: {recall_achieved_recall:.2%}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e910240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION: Precision-Recall Trade-off by Threshold\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Precision and Recall vs Threshold\n",
    "ax1 = axes[0]\n",
    "ax1.plot(thresholds, precisions[:-1], 'b-', label='Precision', linewidth=2)\n",
    "ax1.plot(thresholds, recalls[:-1], 'g-', label='Recall', linewidth=2)\n",
    "ax1.plot(thresholds, f1_scores, 'm-', label='F1 Score', linewidth=2, alpha=0.7)\n",
    "ax1.axhline(y=TARGET_RECALL, color='r', linestyle='--', alpha=0.5, \n",
    "            label=f'Target Recall ({TARGET_RECALL:.0%})')\n",
    "ax1.axvline(x=0.5, color='gray', linestyle=':', alpha=0.5, label='Default (0.5)')\n",
    "ax1.axvline(x=f1_optimal_threshold, color='purple', linestyle='--', alpha=0.7, \n",
    "            label=f'F1-Optimized ({f1_optimal_threshold:.3f})')\n",
    "ax1.axvline(x=recall_optimal_threshold, color='orange', linestyle='--', alpha=0.7, \n",
    "            label=f'Recall-Optimized ({recall_optimal_threshold:.3f})')\n",
    "ax1.set_xlabel('Threshold', fontsize=12)\n",
    "ax1.set_ylabel('Score', fontsize=12)\n",
    "ax1.set_title('XGBoost: Precision, Recall & F1 at Different Thresholds', fontsize=14)\n",
    "ax1.legend(loc='center right', fontsize=9)\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1.05])\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Business Impact - Leavers caught at each threshold\n",
    "ax2 = axes[1]\n",
    "n_actual_leavers = y_val.sum()\n",
    "leavers_caught = recalls[:-1] * n_actual_leavers\n",
    "\n",
    "ax2.plot(thresholds, leavers_caught, 'g-', linewidth=2)\n",
    "ax2.axvline(x=0.5, color='gray', linestyle=':', alpha=0.5, label='Default (0.5)')\n",
    "ax2.axvline(x=f1_optimal_threshold, color='purple', linestyle='--', alpha=0.7,\n",
    "            label=f'F1-Optimized')\n",
    "ax2.axvline(x=recall_optimal_threshold, color='orange', linestyle='--', alpha=0.7,\n",
    "            label=f'Recall-Optimized')\n",
    "ax2.set_xlabel('Threshold', fontsize=12)\n",
    "ax2.set_ylabel('Number of Employees', fontsize=12)\n",
    "ax2.set_title(f'Leavers Caught at Different Thresholds\\n(Total actual leavers: {n_actual_leavers:.0f})', fontsize=14)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd70a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VALIDATION SET EVALUATION - ALL THREE THRESHOLDS\n",
    "# ============================================================\n",
    "\n",
    "# Predictions with all three thresholds\n",
    "y_val_pred_default = (y_val_proba >= 0.5).astype(int)\n",
    "y_val_pred_f1 = (y_val_proba >= f1_optimal_threshold).astype(int)\n",
    "y_val_pred_recall = (y_val_proba >= recall_optimal_threshold).astype(int)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION SET - DEFAULT THRESHOLD (0.5)\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_val, y_val_pred_default, target_names=['Stay', 'Leave']))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"VALIDATION SET - F1-OPTIMIZED THRESHOLD ({f1_optimal_threshold:.3f})\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_val, y_val_pred_f1, target_names=['Stay', 'Leave']))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"VALIDATION SET - RECALL-OPTIMIZED THRESHOLD ({recall_optimal_threshold:.3f})\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_val, y_val_pred_recall, target_names=['Stay', 'Leave']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f326192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST SET EVALUATION - ALL THREE THRESHOLDS\n",
    "# ============================================================\n",
    "\n",
    "# Get test predictions\n",
    "y_test_proba = best_xgb.predict_proba(X_test)[:, 1]\n",
    "y_test_pred_default = (y_test_proba >= 0.5).astype(int)\n",
    "y_test_pred_f1 = (y_test_proba >= f1_optimal_threshold).astype(int)\n",
    "y_test_pred_recall = (y_test_proba >= recall_optimal_threshold).astype(int)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST SET - DEFAULT THRESHOLD (0.5)\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, y_test_pred_default, target_names=['Stay', 'Leave']))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"TEST SET - F1-OPTIMIZED THRESHOLD ({f1_optimal_threshold:.3f})\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, y_test_pred_f1, target_names=['Stay', 'Leave']))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"TEST SET - RECALL-OPTIMIZED THRESHOLD ({recall_optimal_threshold:.3f})\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, y_test_pred_recall, target_names=['Stay', 'Leave']))\n",
    "\n",
    "# Business metrics comparison\n",
    "cm_default = confusion_matrix(y_test, y_test_pred_default)\n",
    "cm_f1 = confusion_matrix(y_test, y_test_pred_f1)\n",
    "cm_recall = confusion_matrix(y_test, y_test_pred_recall)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 75)\n",
    "print(\"BUSINESS IMPACT COMPARISON - TEST SET\")\n",
    "print(\"=\" * 75)\n",
    "print(f\"{'Metric':<30} {'Default':>12} {'F1-Opt':>12} {'Recall-Opt':>12}\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'Threshold':<30} {'0.500':>12} {f1_optimal_threshold:>12.3f} {recall_optimal_threshold:>12.3f}\")\n",
    "print(f\"{'Leavers Caught (TP)':<30} {cm_default[1,1]:>12} {cm_f1[1,1]:>12} {cm_recall[1,1]:>12}\")\n",
    "print(f\"{'Missed Departures (FN)':<30} {cm_default[1,0]:>12} {cm_f1[1,0]:>12} {cm_recall[1,0]:>12}\")\n",
    "print(f\"{'False Alarms (FP)':<30} {cm_default[0,1]:>12} {cm_f1[0,1]:>12} {cm_recall[0,1]:>12}\")\n",
    "print(f\"{'Correct Stay Predictions (TN)':<30} {cm_default[0,0]:>12} {cm_f1[0,0]:>12} {cm_recall[0,0]:>12}\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "# Calculate percentages\n",
    "total_leavers = cm_default[1,0] + cm_default[1,1]\n",
    "print(f\"\\n{'Recall (% Leavers Caught)':<30} {cm_default[1,1]/total_leavers:>11.1%} {cm_f1[1,1]/total_leavers:>12.1%} {cm_recall[1,1]/total_leavers:>12.1%}\")\n",
    "print(f\"{'Precision (% Correct Flags)':<30} {cm_default[1,1]/(cm_default[1,1]+cm_default[0,1]):>11.1%} {cm_f1[1,1]/(cm_f1[1,1]+cm_f1[0,1]):>12.1%} {cm_recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09980a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFUSION MATRICES: Default vs F1-Optimized vs Recall-Optimized\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Default threshold\n",
    "sns.heatmap(cm_default, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Pred: Stay', 'Pred: Leave'],\n",
    "            yticklabels=['Actual: Stay', 'Actual: Leave'],\n",
    "            annot_kws={'size': 14})\n",
    "axes[0].set_title(f'Default Threshold (0.5)\\nMissed: {cm_default[1,0]} | False Alarms: {cm_default[0,1]}', fontsize=12)\n",
    "axes[0].set_ylabel('Actual', fontsize=11)\n",
    "axes[0].set_xlabel('Predicted', fontsize=11)\n",
    "\n",
    "# F1-optimized threshold\n",
    "sns.heatmap(cm_f1, annot=True, fmt='d', cmap='Purples', ax=axes[1],\n",
    "            xticklabels=['Pred: Stay', 'Pred: Leave'],\n",
    "            yticklabels=['Actual: Stay', 'Actual: Leave'],\n",
    "            annot_kws={'size': 14})\n",
    "axes[1].set_title(f'F1-Optimized ({f1_optimal_threshold:.3f})\\nMissed: {cm_f1[1,0]} | False Alarms: {cm_f1[0,1]}', fontsize=12)\n",
    "axes[1].set_ylabel('Actual', fontsize=11)\n",
    "axes[1].set_xlabel('Predicted', fontsize=11)\n",
    "\n",
    "# Recall-optimized threshold\n",
    "sns.heatmap(cm_recall, annot=True, fmt='d', cmap='Greens', ax=axes[2],\n",
    "            xticklabels=['Pred: Stay', 'Pred: Leave'],\n",
    "            yticklabels=['Actual: Stay', 'Actual: Leave'],\n",
    "            annot_kws={'size': 14})\n",
    "axes[2].set_title(f'Recall-Optimized ({recall_optimal_threshold:.3f})\\nMissed: {cm_recall[1,0]} | False Alarms: {cm_recall[0,1]}', fontsize=12)\n",
    "axes[2].set_ylabel('Actual', fontsize=11)\n",
    "axes[2].set_xlabel('Predicted', fontsize=11)\n",
    "\n",
    "plt.suptitle('XGBoost - Confusion Matrices by Threshold', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093f07b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ROC CURVE\n",
    "# ============================================================\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_test_proba)\n",
    "auc_score = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, 'b-', linewidth=2, label=f'XGBoost (AUC = {auc_score:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve - XGBoost (Test Set)', fontsize=14)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4f64a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FEATURE IMPORTANCE (XGBoost Native)\n",
    "# ============================================================\n",
    "\n",
    "# Get feature importance\n",
    "feature_names = X_train.columns\n",
    "importances = best_xgb.feature_importances_\n",
    "\n",
    "# Create DataFrame and sort\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot top 15 features\n",
    "top_n = 15\n",
    "top_features = feature_importance.head(top_n)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(top_features)))\n",
    "plt.barh(range(len(top_features)), top_features['Importance'], color=colors)\n",
    "plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "plt.xlabel('Importance (Gain)', fontsize=12)\n",
    "plt.title('XGBoost - Top 15 Feature Importances', fontsize=14)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print feature importance table\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FEATURE IMPORTANCE RANKING\")\n",
    "print(\"=\" * 50)\n",
    "for i, row in feature_importance.iterrows():\n",
    "    print(f\"{row['Feature']:<40} {row['Importance']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa33ae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE MODEL AND THRESHOLDS\n",
    "# ============================================================\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create models directory\n",
    "MODELS_PATH = 'models/'\n",
    "os.makedirs(MODELS_PATH, exist_ok=True)\n",
    "\n",
    "# Save the trained model\n",
    "model_filename = os.path.join(MODELS_PATH, 'xgboost_attrition_model.joblib')\n",
    "joblib.dump(best_xgb, model_filename)\n",
    "print(f\"Model saved to: {model_filename}\")\n",
    "\n",
    "# Save thresholds and metadata\n",
    "model_metadata = {\n",
    "    'model_type': 'XGBoost',\n",
    "    'best_params': grid_search.best_params_,\n",
    "    'cv_f1_score': float(grid_search.best_score_),\n",
    "    'test_auc_score': float(auc_score),\n",
    "    'thresholds': {\n",
    "        'default': 0.5,\n",
    "        'f1_optimized': float(f1_optimal_threshold),\n",
    "        'recall_optimized': float(recall_optimal_threshold)\n",
    "    },\n",
    "    'target_recall': float(TARGET_RECALL),\n",
    "    'feature_names': list(X_train.columns),\n",
    "    'feature_importance': feature_importance.to_dict('records')\n",
    "}\n",
    "\n",
    "metadata_filename = os.path.join(MODELS_PATH, 'xgboost_model_metadata.json')\n",
    "with open(metadata_filename, 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=4)\n",
    "print(f\"Metadata saved to: {metadata_filename}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
