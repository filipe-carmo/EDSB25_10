{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9274bf46",
   "metadata": {},
   "source": [
    "## Notes about the model:\n",
    "\n",
    "- **Parametric supervised learning method** that models the probability of a binary outcome (Attrition) using a logistic function\n",
    "- The algorithm predicts the likelihood of employee attrition by learning a linear combination of features, transformed through the sigmoid function to output probabilities between 0 and 1\n",
    "- **Handles both numeric and categorical variables**, though scikit-learn implementation requires encoding categorical features as dummy variables (one-hot encoding), similar to other models\n",
    "- Logistic Regression can suffer from **multicollinearity** when features are highly correlated, which can make coefficient interpretation unreliable and model performance unstable\n",
    "- The model performs well when there is a **linear relationship between the log-odds of the outcome and the features**. Non-linear relationships may require feature engineering or polynomial terms\n",
    "- **Class imbalance** is a significant challenge in this attrition problem, requiring techniques like SMOTE (Synthetic Minority Oversampling Technique) to improve recall for the minority class\n",
    "- **Regularization parameters** (L1 or L2 penalties via the `C` parameter) help prevent overfitting by penalizing large coefficients, especially important when dealing with many features\n",
    "- **Class weights** can be tuned to handle imbalanced datasets by assigning higher penalties to misclassifying the minority class\n",
    "- Model provides **interpretable coefficients** that indicate the direction and strength of each feature's impact on attrition probability, making it valuable for business understanding\n",
    "\n",
    "**Support:** [https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffab66b3",
   "metadata": {},
   "source": [
    "## Quick Overview: Logistic Regression for HR Attrition Prediction\n",
    "\n",
    "This notebook implements a Logistic Regression model to predict employee attrition with a focus on handling class imbalance and optimizing probability thresholds for business use. It serves as a strong, interpretable baseline that performs very well in the global evaluation.\n",
    "\n",
    "### Key Approach\n",
    "\n",
    "- **Data Preprocessing**: Applies the shared preprocessing pipeline with imputation, scaling, encoding, and feature engineering (18 features total).\n",
    "- **Class Imbalance Handling**: Uses SMOTE (Synthetic Minority Oversampling Technique) to increase the minority (leaver) representation in the training data from 16.2% to 33.3%.\n",
    "- **Hyperparameter Tuning**: Runs GridSearchCV with 5‑fold StratifiedKFold across 50 parameter combinations to tune regularization strength, penalty, solver, and class weights.\n",
    "- **Threshold Optimization**: Implements a dual threshold strategy on predicted probabilities:\n",
    "  - **F1‑Optimized** (0.578): best balance between precision and recall.\n",
    "  - **Recall‑Optimized** (0.400): prioritizes catching as many leavers as possible at the cost of more false positives (business‑focused setting).\n",
    "\n",
    "### Results\n",
    "\n",
    "**Best Model Configuration:**\n",
    "\n",
    "- C: 0.1  \n",
    "- Class Weight: `{0: 1, 1: 2}`  \n",
    "- Penalty: L2  \n",
    "- Solver: `lbfgs`  \n",
    "- CV F1 Score: 0.7164  \n",
    "\n",
    "**Test Set Performance (Logistic Regression):**\n",
    "\n",
    "| Threshold              | Recall (Leavers) | Precision (Leavers) | Accuracy | Use Case                                      |\n",
    "|------------------------|------------------|----------------------|----------|-----------------------------------------------|\n",
    "| Default (0.50)         | 63.8%            | 39.5%                | 79%      | Baseline probability cutoff                   |\n",
    "| F1‑Optimized (0.578)   | 53.2%            | 44.6%                | 82%      | Best overall F1 on test set                   |\n",
    "| **Recall‑Optimized (0.400)** | **76.6%**    | **34.0%**            | 72%      | **Minimize missed departures (business focus)** |\n",
    "\n",
    "At the recall‑optimized threshold, the model misses 11 leavers on the test set (out of 47), with 70 false alarms, offering a strong balance between catching leavers and controlling the number of flagged employees.\n",
    "\n",
    "### Business Impact\n",
    "\n",
    "The **Recall‑Optimized Logistic Regression model** is the top performer in the global evaluation at the business‑priority threshold, combining high recall with the best overall mix of precision, F1, and accuracy. It was selected as the primary deployment model, providing interpretable coefficients and clear probability scores that support proactive retention decisions and transparent HR communication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675540ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca86148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, confusion_matrix, \n",
    "                             precision_recall_curve, classification_report)\n",
    "import joblib\n",
    "import json\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Get the current notebook directory (notebooks folder)\n",
    "base_path = Path.cwd()\n",
    "\n",
    "# Get the project root (parent of notebooks folder)\n",
    "project_root = base_path.parent\n",
    "\n",
    "# Add the project root to Python path to find the scripts module\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import functions and feature list\n",
    "from preprocessing import preprocess, selected_features\n",
    "\n",
    "# SMOTE for oversampling\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb1863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(y_true, y_proba, title='ROC Curve'):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "    plt.plot([0,1], [0,1],'k--')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_conf_matrix(y_true, y_pred, title='Confusion Matrix'):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(4,3))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5d62a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Preprocessing ---\n",
    "DATA_PATH = project_root / \"data\" / \"raw\" / \"HR_Attrition_Dataset.csv\"\n",
    "\n",
    "# Call Preprocessing function to return split dataset\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, scaler, imputer = preprocess(data_path=DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f07ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# APPLY SMOTE + LOGISTIC REGRESSION HYPERPARAMETER TUNING\n",
    "# ============================================\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE to training data before GridSearchCV\n",
    "smote = SMOTE(sampling_strategy=0.5, random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SMOTE OVERSAMPLING RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training set before SMOTE: {X_train.shape[0]} samples\")\n",
    "print(f\"Training set after SMOTE: {X_train_smote.shape[0]} samples\")\n",
    "print(f\"\\nClass distribution before SMOTE:\")\n",
    "print(y_train.value_counts(normalize=True).round(3))\n",
    "print(f\"\\nClass distribution after SMOTE:\")\n",
    "print(pd.Series(y_train_smote).value_counts(normalize=True).round(3))\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set up StratifiedKFold\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Expanded hyperparameter grid with class_weight tuning\n",
    "# Note: With SMOTE, class_weight becomes less critical but still worth tuning\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['lbfgs', 'liblinear'],\n",
    "    'class_weight': [None, 'balanced', {0: 1, 1: 2}, {0: 1, 1: 3}, {0: 1, 1: 5}]\n",
    "}\n",
    "\n",
    "# Base classifier\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# GridSearchCV with F1 scoring (for hyperparameters more balanced)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=clf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1',\n",
    "    cv=skf,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit model with SMOTE-resampled data\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RUNNING GRIDSEARCHCV WITH SMOTE DATA\")\n",
    "print(\"=\" * 60)\n",
    "grid_search.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GRIDSEARCHCV RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best F1 Score (CV): {grid_search.best_score_:.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Best estimator\n",
    "best_logreg = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385277e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Get Cross-Validation Results ---\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "leaderboard = cv_results.loc[:, ['params', 'mean_test_score', 'std_test_score']]\n",
    "leaderboard = leaderboard.rename(columns={'mean_test_score': 'mean_F1', 'std_test_score': 'std_F1'})\n",
    "leaderboard = leaderboard.sort_values(by=\"mean_F1\", ascending=False)\n",
    "print(\"\\nTop 10 Hyperparameter Combinations:\")\n",
    "print(leaderboard.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0385f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# THRESHOLD OPTIMIZATION - DUAL STRATEGY\n",
    "# 1. F1-Optimized: Best balance between precision and recall\n",
    "# 2. Recall-Optimized: Minimize surprise departures\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Get predicted probabilities on validation set\n",
    "y_val_proba = best_logreg.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Calculate precision-recall curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_val, y_val_proba)\n",
    "\n",
    "# ---- F1-OPTIMIZED THRESHOLD ----\n",
    "# Calculate F1 score at each threshold\n",
    "f1_scores = 2 * (precisions[:-1] * recalls[:-1]) / (precisions[:-1] + recalls[:-1] + 1e-10)\n",
    "best_f1_idx = np.argmax(f1_scores)\n",
    "f1_optimal_threshold = thresholds[best_f1_idx]\n",
    "f1_achieved_recall = recalls[best_f1_idx]\n",
    "f1_achieved_precision = precisions[best_f1_idx]\n",
    "f1_achieved_f1 = f1_scores[best_f1_idx]\n",
    "\n",
    "# ---- RECALL-OPTIMIZED THRESHOLD ----\n",
    "TARGET_RECALL = 0.70  \n",
    "\n",
    "valid_indices = np.where(recalls[:-1] >= TARGET_RECALL)[0]\n",
    "\n",
    "if len(valid_indices) > 0:\n",
    "    best_idx = valid_indices[np.argmax(precisions[:-1][valid_indices])]\n",
    "    recall_optimal_threshold = thresholds[best_idx]\n",
    "    recall_achieved_recall = recalls[best_idx]\n",
    "    recall_achieved_precision = precisions[best_idx]\n",
    "else:\n",
    "    recall_optimal_threshold = thresholds[0]\n",
    "    recall_achieved_recall = recalls[0]\n",
    "    recall_achieved_precision = precisions[0]\n",
    "    print(f\"WARNING: Target recall {TARGET_RECALL:.0%} not achievable. Using lowest threshold.\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"THRESHOLD OPTIMIZATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'F1-OPTIMIZED THRESHOLD':^60}\")\n",
    "print(f\"Threshold: {f1_optimal_threshold:.3f}\")\n",
    "print(f\"Precision: {f1_achieved_precision:.2%}\")\n",
    "print(f\"Recall: {f1_achieved_recall:.2%}\")\n",
    "print(f\"F1 Score: {f1_achieved_f1:.3f}\")\n",
    "\n",
    "print(f\"\\n{'RECALL-OPTIMIZED THRESHOLD':^60}\")\n",
    "print(f\"Target Recall: {TARGET_RECALL:.0%}\")\n",
    "print(f\"Threshold: {recall_optimal_threshold:.3f}\")\n",
    "print(f\"Precision: {recall_achieved_precision:.2%}\")\n",
    "print(f\"Recall: {recall_achieved_recall:.2%}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# After testing we decided to override with more aggressive threshold (the difference wouldn't be agressive enough)\n",
    "recall_optimal_threshold = 0.40\n",
    "\n",
    "print(f\"OVERRIDDEN Recall-Optimized Threshold: {recall_optimal_threshold:.3f}\")\n",
    "print(f\"(Manual override to account for validation-test gap)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5532f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION: Precision-Recall Trade-off by Threshold\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Precision and Recall vs Threshold\n",
    "ax1 = axes[0]\n",
    "ax1.plot(thresholds, precisions[:-1], 'b-', label='Precision', linewidth=2)\n",
    "ax1.plot(thresholds, recalls[:-1], 'g-', label='Recall', linewidth=2)\n",
    "ax1.plot(thresholds, f1_scores, 'm-', label='F1 Score', linewidth=2, alpha=0.7)\n",
    "ax1.axhline(y=TARGET_RECALL, color='r', linestyle='--', alpha=0.5, \n",
    "            label=f'Target Recall ({TARGET_RECALL:.0%})')\n",
    "ax1.axvline(x=0.5, color='gray', linestyle=':', alpha=0.5, label='Default (0.5)')\n",
    "ax1.axvline(x=f1_optimal_threshold, color='purple', linestyle='--', alpha=0.7, \n",
    "            label=f'F1-Optimized ({f1_optimal_threshold:.3f})')\n",
    "ax1.axvline(x=recall_optimal_threshold, color='orange', linestyle='--', alpha=0.7, \n",
    "            label=f'Recall-Optimized ({recall_optimal_threshold:.3f})')\n",
    "ax1.set_xlabel('Threshold', fontsize=12)\n",
    "ax1.set_ylabel('Score', fontsize=12)\n",
    "ax1.set_title('Precision, Recall & F1 at Different Thresholds', fontsize=14)\n",
    "ax1.legend(loc='center right', fontsize=9)\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1.05])\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Business Impact - Leavers caught at each threshold\n",
    "ax2 = axes[1]\n",
    "n_actual_leavers = y_val.sum()\n",
    "leavers_caught = recalls[:-1] * n_actual_leavers\n",
    "\n",
    "ax2.plot(thresholds, leavers_caught, 'g-', linewidth=2)\n",
    "ax2.axvline(x=0.5, color='gray', linestyle=':', alpha=0.5, label='Default (0.5)')\n",
    "ax2.axvline(x=f1_optimal_threshold, color='purple', linestyle='--', alpha=0.7,\n",
    "            label=f'F1-Optimized')\n",
    "ax2.axvline(x=recall_optimal_threshold, color='orange', linestyle='--', alpha=0.7,\n",
    "            label=f'Recall-Optimized')\n",
    "ax2.set_xlabel('Threshold', fontsize=12)\n",
    "ax2.set_ylabel('Number of Employees', fontsize=12)\n",
    "ax2.set_title(f'Leavers Caught at Different Thresholds\\n(Total actual leavers: {n_actual_leavers:.0f})', fontsize=14)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793b9f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VALIDATION SET EVALUATION - ALL THREE THRESHOLDS\n",
    "# ============================================================\n",
    "\n",
    "# Predictions with all three thresholds\n",
    "y_val_pred_default = (y_val_proba >= 0.5).astype(int)\n",
    "y_val_pred_f1 = (y_val_proba >= f1_optimal_threshold).astype(int)\n",
    "y_val_pred_recall = (y_val_proba >= recall_optimal_threshold).astype(int)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION SET - DEFAULT THRESHOLD (0.5)\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_val, y_val_pred_default, target_names=['Stay', 'Leave']))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"VALIDATION SET - F1-OPTIMIZED THRESHOLD ({f1_optimal_threshold:.3f})\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_val, y_val_pred_f1, target_names=['Stay', 'Leave']))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"VALIDATION SET - RECALL-OPTIMIZED THRESHOLD ({recall_optimal_threshold:.3f})\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_val, y_val_pred_recall, target_names=['Stay', 'Leave']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df5b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST SET EVALUATION - ALL THREE THRESHOLDS\n",
    "# ============================================================\n",
    "\n",
    "# Get test predictions\n",
    "y_test_proba = best_logreg.predict_proba(X_test)[:, 1]\n",
    "y_test_pred_default = (y_test_proba >= 0.5).astype(int)\n",
    "y_test_pred_f1 = (y_test_proba >= f1_optimal_threshold).astype(int)\n",
    "y_test_pred_recall = (y_test_proba >= recall_optimal_threshold).astype(int)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST SET - DEFAULT THRESHOLD (0.5)\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, y_test_pred_default, target_names=['Stay', 'Leave']))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"TEST SET - F1-OPTIMIZED THRESHOLD ({f1_optimal_threshold:.3f})\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, y_test_pred_f1, target_names=['Stay', 'Leave']))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"TEST SET - RECALL-OPTIMIZED THRESHOLD ({recall_optimal_threshold:.3f})\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, y_test_pred_recall, target_names=['Stay', 'Leave']))\n",
    "\n",
    "# Business metrics comparison\n",
    "cm_default = confusion_matrix(y_test, y_test_pred_default)\n",
    "cm_f1 = confusion_matrix(y_test, y_test_pred_f1)\n",
    "cm_recall = confusion_matrix(y_test, y_test_pred_recall)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 75)\n",
    "print(\"BUSINESS IMPACT COMPARISON - TEST SET\")\n",
    "print(\"=\" * 75)\n",
    "print(f\"{'Metric':<30} {'Default':>12} {'F1-Opt':>12} {'Recall-Opt':>12}\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'Threshold':<30} {'0.500':>12} {f1_optimal_threshold:>12.3f} {recall_optimal_threshold:>12.3f}\")\n",
    "print(f\"{'Leavers Caught (TP)':<30} {cm_default[1,1]:>12} {cm_f1[1,1]:>12} {cm_recall[1,1]:>12}\")\n",
    "print(f\"{'Missed Departures (FN)':<30} {cm_default[1,0]:>12} {cm_f1[1,0]:>12} {cm_recall[1,0]:>12}\")\n",
    "print(f\"{'False Alarms (FP)':<30} {cm_default[0,1]:>12} {cm_f1[0,1]:>12} {cm_recall[0,1]:>12}\")\n",
    "print(f\"{'Correct Stay Predictions (TN)':<30} {cm_default[0,0]:>12} {cm_f1[0,0]:>12} {cm_recall[0,0]:>12}\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "# Calculate percentages from the confusion matrices and create visual table\n",
    "total_leavers = cm_default[1,0] + cm_default[1,1]\n",
    "print(f\"\\n{'Recall (% Leavers Caught)':<30} {cm_default[1,1]/total_leavers:>11.1%} {cm_f1[1,1]/total_leavers:>12.1%} {cm_recall[1,1]/total_leavers:>12.1%}\")\n",
    "print(f\"{'Precision (% Correct Flags)':<30} {cm_default[1,1]/(cm_default[1,1]+cm_default[0,1]):>11.1%} {cm_f1[1,1]/(cm_f1[1,1]+cm_f1[0,1]):>12.1%} {cm_recall[1,1]/(cm_recall[1,1]+cm_recall[0,1]):>12.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c50e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZE CONFUSION MATRICES WITH PERCENTAGES + ROC CURVE\n",
    "# ============================================\n",
    "\n",
    "fig = plt.figure(figsize=(20, 5))\n",
    "gs = fig.add_gridspec(1, 4, width_ratios=[1, 1, 1, 1.2])\n",
    "\n",
    "# First 3 subplots for confusion matrices\n",
    "axes = [fig.add_subplot(gs[0, i]) for i in range(3)]\n",
    "\n",
    "# Calculate confusion matrices\n",
    "cm_default = confusion_matrix(y_test, y_test_pred_default)\n",
    "cm_f1 = confusion_matrix(y_test, y_test_pred_f1)\n",
    "cm_recall = confusion_matrix(y_test, y_test_pred_recall)\n",
    "\n",
    "thresholds_data = [\n",
    "    (\"Default (0.50)\", y_test_pred_default, cm_default, 'Blues'),\n",
    "    (f\"F1-Opt ({f1_optimal_threshold:.2f})\", y_test_pred_f1, cm_f1, 'Purples'),\n",
    "    (f\"Recall-Opt ({recall_optimal_threshold:.2f})\", y_test_pred_recall, cm_recall, 'Greens')\n",
    "]\n",
    "\n",
    "for idx, (title, y_pred, cm, cmap) in enumerate(thresholds_data):\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    \n",
    "    # Create annotations with counts and percentages\n",
    "    annot = np.array([[f'{cm[i,j]}\\n({cm_percent[i,j]:.1f}%)' \n",
    "                       for j in range(cm.shape[1])] \n",
    "                      for i in range(cm.shape[0])])\n",
    "    \n",
    "    sns.heatmap(cm, annot=annot, fmt='', cmap=cmap,\n",
    "                xticklabels=['Stay', 'Leave'],\n",
    "                yticklabels=['Stay', 'Leave'],\n",
    "                cbar=False, ax=axes[idx],\n",
    "                annot_kws={'size': 12, 'weight': 'bold'})\n",
    "    \n",
    "    axes[idx].set_title(title, fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Predicted', fontsize=10)\n",
    "    axes[idx].set_ylabel('Actual', fontsize=10)\n",
    "    \n",
    "    # Add recall at bottom\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    axes[idx].text(0.5, -0.15, f'Recall: {recall:.1%}', \n",
    "                   transform=axes[idx].transAxes,\n",
    "                   fontsize=11, fontweight='bold',\n",
    "                   horizontalalignment='center')\n",
    "\n",
    "# Fourth subplot for ROC curve\n",
    "ax_roc = fig.add_subplot(gs[0, 3])\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_test, y_test_proba)\n",
    "auc_score = roc_auc_score(y_test, y_test_proba)\n",
    "print(auc_score)\n",
    "\n",
    "# Plot ROC curve\n",
    "ax_roc.plot(fpr, tpr, color='darkblue', lw=2, label=f'ROC curve (AUC = {auc_score:.3f})')\n",
    "ax_roc.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "\n",
    "# Mark the threshold points\n",
    "for threshold_name, y_pred, color, marker in [\n",
    "    (\"Default\", y_test_pred_default, 'red', 'o'),\n",
    "    (\"F1-Opt\", y_test_pred_f1, 'green', 's'),\n",
    "    (\"Recall-Opt\", y_test_pred_recall, 'purple', '^')\n",
    "]:\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    fpr_point = cm[0, 1] / (cm[0, 0] + cm[0, 1])  # FP / (TN + FP)\n",
    "    tpr_point = cm[1, 1] / (cm[1, 0] + cm[1, 1])  # TP / (FN + TP)\n",
    "    ax_roc.scatter([fpr_point], [tpr_point], s=100, c=color, marker=marker, \n",
    "                   edgecolors='black', linewidths=1.5, label=threshold_name, zorder=5)\n",
    "\n",
    "ax_roc.set_xlim([0.0, 1.0])\n",
    "ax_roc.set_ylim([0.0, 1.05])\n",
    "ax_roc.set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')\n",
    "ax_roc.set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')\n",
    "ax_roc.set_title('ROC Curve', fontsize=12, fontweight='bold')\n",
    "ax_roc.legend(loc='lower right', fontsize=9)\n",
    "ax_roc.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Logistic Regression - Confusion Matrices & ROC Curve (Test Set)', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306b56dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from coefficients\n",
    "feature_names = X_train.columns\n",
    "coefficients = best_logreg.coef_[0]\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': coefficients,\n",
    "    'abs_coefficient': np.abs(coefficients)\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "print(\"\\n--- Top 10 Most Important Features ---\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features = feature_importance.head(10)\n",
    "colors = ['green' if c > 0 else 'red' for c in top_features['coefficient']]\n",
    "plt.barh(top_features['feature'][::-1], top_features['coefficient'][::-1], color=colors[::-1])\n",
    "plt.xlabel('Coefficient')\n",
    "plt.title('Top 10 Feature Importance (Logistic Regression Coefficients)')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f04058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SAVE MODEL AND METADATA\n",
    "# ============================================\n",
    "\n",
    "# Create models directory at project root\n",
    "MODEL_PATH = project_root / 'models'\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the trained model\n",
    "model_filename = MODEL_PATH / 'logistic_regression_best.pkl'\n",
    "joblib.dump(best_logreg, model_filename)\n",
    "\n",
    "# Save scaler and imputer (needed for deployment)\n",
    "joblib.dump(scaler, MODEL_PATH / 'scaler.pkl')\n",
    "joblib.dump(imputer, MODEL_PATH / 'imputer.pkl')\n",
    "print(f\"✓ Model saved to {model_filename}\")\n",
    "\n",
    "# Save thresholds and metadata\n",
    "model_metadata = {\n",
    "    'model_type': 'Logistic Regression',\n",
    "    'best_params': grid_search.best_params_,\n",
    "    'cv_f1_score': float(grid_search.best_score_),\n",
    "    'test_auc_score': float(auc_score),\n",
    "    'thresholds': {\n",
    "        'default': 0.5,\n",
    "        'f1_optimized': float(f1_optimal_threshold),\n",
    "        'recall_optimized': float(recall_optimal_threshold)\n",
    "    },\n",
    "    'target_recall': float(TARGET_RECALL),\n",
    "    'feature_names': list(X_train.columns)\n",
    "}\n",
    "\n",
    "metadata_filename = MODEL_PATH / 'logreg_model_metadata.json'\n",
    "with open(metadata_filename, 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=4)\n",
    "print(f\"✓ Metadata saved to {metadata_filename}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"MODEL ARTIFACTS SAVED\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Model file: {model_filename}\")\n",
    "print(f\"Metadata file: {metadata_filename}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
